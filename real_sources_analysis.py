#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
An√°lisis REAL de compatibilidad entre fuentes basado en archivos JSON existentes
NO INVENTA NADA - Solo analiza datos reales de los archivos descargados
"""

import json
import os
import csv
from typing import Dict, List, Any
from datetime import datetime
import re

class SourceDataAnalyzer:
    """Analizador de datos reales por fuente."""
    
    def __init__(self):
        self.base_path = "/Users/luisfer/Documents/Proyectos IA/paloma-new/paloma-licitera-new/data/raw"
        self.analysis = {
            'DOF': {'fields': set(), 'samples': [], 'structure': {}},
            'ComprasMX': {'fields': set(), 'samples': [], 'structure': {}},
            'TianguisDigital': {'fields': set(), 'samples': [], 'structure': {}}
        }
    
    def analyze_dof_files(self):
        """Analizar archivos reales del DOF."""
        print("üîç Analizando archivos REALES del DOF...")
        
        dof_path = os.path.join(self.base_path, "dof")
        json_files = [f for f in os.listdir(dof_path) if f.endswith('_licitaciones.json')]
        
        for json_file in json_files[:2]:  # Solo 2 archivos para an√°lisis
            file_path = os.path.join(dof_path, json_file)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                print(f"  üìÑ Archivo: {json_file}")
                print(f"     Total licitaciones: {data.get('total_licitaciones', 0)}")
                
                # Analizar estructura de licitaciones
                if 'licitaciones' in data and data['licitaciones']:
                    sample_licit = data['licitaciones'][0]
                    
                    # Campos disponibles
                    campos = list(sample_licit.keys())
                    self.analysis['DOF']['fields'].update(campos)
                    
                    # Guardar muestra
                    self.analysis['DOF']['samples'].append(sample_licit)
                    
                    print(f"     Campos encontrados: {len(campos)}")
                    print(f"     Campos: {', '.join(campos[:8])}{'...' if len(campos) > 8 else ''}")
                    
                    # Analizar tipos de datos
                    for campo, valor in sample_licit.items():\n                        if valor is not None:\n                            tipo = type(valor).__name__\n                            if campo not in self.analysis['DOF']['structure']:\n                                self.analysis['DOF']['structure'][campo] = {'tipos': set(), 'ejemplos': []}\n                            self.analysis['DOF']['structure'][campo]['tipos'].add(tipo)\n                            if len(self.analysis['DOF']['structure'][campo]['ejemplos']) < 3:\n                                ejemplo = str(valor)[:100] + '...' if len(str(valor)) > 100 else str(valor)\n                                self.analysis['DOF']['structure'][campo]['ejemplos'].append(ejemplo)\n                                \n            except Exception as e:\n                print(f\"  ‚ùå Error leyendo {json_file}: {e}\")\n        \n        print(f\"  ‚úÖ DOF: {len(self.analysis['DOF']['fields'])} campos √∫nicos encontrados\")\n    \n    def analyze_comprasmx_files(self):\n        \"\"\"Analizar archivos reales de ComprasMX.\"\"\"\n        print(\"\\nüîç Analizando archivos REALES de ComprasMX...\")\n        \n        comprasmx_path = os.path.join(self.base_path, \"comprasmx\")\n        \n        # Buscar archivo principal de expedientes\n        main_file = None\n        for f in os.listdir(comprasmx_path):\n            if 'todos_expedientes' in f and f.endswith('.json'):\n                main_file = f\n                break\n        \n        if not main_file:\n            print(\"  ‚ùå No se encontr√≥ archivo de expedientes de ComprasMX\")\n            return\n        \n        file_path = os.path.join(comprasmx_path, main_file)\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n            \n            print(f\"  üìÑ Archivo: {main_file}\")\n            print(f\"     Total expedientes: {data.get('total_expedientes', 0)}\")\n            \n            # Analizar estructura de expedientes\n            if 'expedientes' in data and data['expedientes']:\n                sample_exp = data['expedientes'][0]\n                \n                # Campos disponibles\n                campos = list(sample_exp.keys())\n                self.analysis['ComprasMX']['fields'].update(campos)\n                \n                # Guardar muestra\n                self.analysis['ComprasMX']['samples'].append(sample_exp)\n                \n                print(f\"     Campos encontrados: {len(campos)}\")\n                print(f\"     Campos: {', '.join(campos[:8])}{'...' if len(campos) > 8 else ''}\")\n                \n                # Analizar tipos de datos\n                for campo, valor in sample_exp.items():\n                    if valor is not None:\n                        tipo = type(valor).__name__\n                        if campo not in self.analysis['ComprasMX']['structure']:\n                            self.analysis['ComprasMX']['structure'][campo] = {'tipos': set(), 'ejemplos': []}\n                        self.analysis['ComprasMX']['structure'][campo]['tipos'].add(tipo)\n                        if len(self.analysis['ComprasMX']['structure'][campo]['ejemplos']) < 3:\n                            ejemplo = str(valor)[:100] + '...' if len(str(valor)) > 100 else str(valor)\n                            self.analysis['ComprasMX']['structure'][campo]['ejemplos'].append(ejemplo)\n                            \n        except Exception as e:\n            print(f\"  ‚ùå Error leyendo {main_file}: {e}\")\n        \n        print(f\"  ‚úÖ ComprasMX: {len(self.analysis['ComprasMX']['fields'])} campos √∫nicos encontrados\")\n    \n    def analyze_tianguis_files(self):\n        \"\"\"Analizar archivos reales de Tianguis Digital.\"\"\"\n        print(\"\\nüîç Analizando archivos REALES de Tianguis Digital...\")\n        \n        tianguis_path = os.path.join(self.base_path, \"tianguis\")\n        csv_files = [f for f in os.listdir(tianguis_path) if f.endswith('.csv') and not f.endswith('_new.csv')]\n        \n        if not csv_files:\n            print(\"  ‚ùå No se encontraron archivos CSV de Tianguis Digital\")\n            return\n        \n        # Analizar primer CSV\n        csv_file = csv_files[0]\n        file_path = os.path.join(tianguis_path, csv_file)\n        \n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                reader = csv.DictReader(f)\n                \n                # Obtener headers\n                headers = reader.fieldnames\n                self.analysis['TianguisDigital']['fields'].update(headers)\n                \n                print(f\"  üìÑ Archivo: {csv_file}\")\n                print(f\"     Campos encontrados: {len(headers)}\")\n                print(f\"     Primeros campos: {', '.join(headers[:8])}{'...' if len(headers) > 8 else ''}\")\n                \n                # Leer algunas muestras\n                samples = []\n                for i, row in enumerate(reader):\n                    if i >= 3:  # Solo 3 muestras\n                        break\n                    samples.append(row)\n                    \n                    # Analizar tipos de datos de la primera muestra\n                    if i == 0:\n                        for campo, valor in row.items():\n                            if valor and valor.strip():  # Solo valores no vac√≠os\n                                tipo = 'str'  # CSV siempre son strings\n                                if campo not in self.analysis['TianguisDigital']['structure']:\n                                    self.analysis['TianguisDigital']['structure'][campo] = {'tipos': set(), 'ejemplos': []}\n                                self.analysis['TianguisDigital']['structure'][campo]['tipos'].add(tipo)\n                                if len(self.analysis['TianguisDigital']['structure'][campo]['ejemplos']) < 3:\n                                    ejemplo = str(valor)[:100] + '...' if len(str(valor)) > 100 else str(valor)\n                                    self.analysis['TianguisDigital']['structure'][campo]['ejemplos'].append(ejemplo)\n                \n                self.analysis['TianguisDigital']['samples'] = samples\n                print(f\"     Muestras analizadas: {len(samples)}\")\n                \n        except Exception as e:\n            print(f\"  ‚ùå Error leyendo {csv_file}: {e}\")\n        \n        print(f\"  ‚úÖ Tianguis: {len(self.analysis['TianguisDigital']['fields'])} campos √∫nicos encontrados\")\n    \n    def compare_sources(self):\n        \"\"\"Comparar qu√© informaci√≥n est√° disponible en cada fuente.\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"üìä COMPARACI√ìN REAL DE CAMPOS DISPONIBLES POR FUENTE\")\n        print(\"=\" * 80)\n        \n        # Mapear campos similares entre fuentes\n        campo_mappings = {\n            'titulo': {\n                'DOF': ['objeto_licitacion'],\n                'ComprasMX': ['nombre_procedimiento'],\n                'TianguisDigital': ['tender/title']\n            },\n            'numero_procedimiento': {\n                'DOF': ['numero_licitacion'],\n                'ComprasMX': ['numero_procedimiento'],\n                'TianguisDigital': ['tender/id']\n            },\n            'entidad': {\n                'DOF': ['dependencia', 'subdependencia'],\n                'ComprasMX': ['unidad_compradora'],\n                'TianguisDigital': ['buyer/name']\n            },\n            'fechas': {\n                'DOF': ['fecha_publicacion', 'fecha_junta_aclaraciones', 'fecha_presentacion_apertura', 'fecha_fallo'],\n                'ComprasMX': ['fecha_aclaraciones', 'fecha_apertura'],\n                'TianguisDigital': ['tender/tenderPeriod/startDate', 'tender/tenderPeriod/endDate']\n            },\n            'monto': {\n                'DOF': [],  # No parece tener montos estructurados\n                'ComprasMX': [],  # No vi montos en la muestra\n                'TianguisDigital': ['tender/value/amount', 'awards/value/amount']\n            },\n            'tipo': {\n                'DOF': [],\n                'ComprasMX': ['tipo_procedimiento', 'tipo_contratacion', 'caracter'],\n                'TianguisDigital': ['tender/procurementMethod']\n            },\n            'ubicacion': {\n                'DOF': [],  # Necesita parsing del texto\n                'ComprasMX': ['entidad_federativa_contratacion'],\n                'TianguisDigital': []  # No evidente en la estructura\n            }\n        }\n        \n        print(\"\\nüîç MAPEO DE CAMPOS EQUIVALENTES:\")\n        print(\"-\" * 60)\n        \n        for concepto, mappings in campo_mappings.items():\n            print(f\"\\nüìã {concepto.upper()}:\")\n            for fuente, campos in mappings.items():\n                if campos:\n                    campos_existentes = []\n                    for campo in campos:\n                        if campo in self.analysis[fuente]['fields']:\n                            campos_existentes.append(f\"‚úÖ {campo}\")\n                        else:\n                            campos_existentes.append(f\"‚ùå {campo}\")\n                    print(f\"  {fuente:<15}: {', '.join(campos_existentes)}\")\n                else:\n                    print(f\"  {fuente:<15}: ‚ùå No identificados\")\n    \n    def analyze_parsing_requirements(self):\n        \"\"\"Analizar qu√© parsing se necesita para cada fuente.\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"üîß AN√ÅLISIS DE REQUERIMIENTOS DE PARSING POR FUENTE\")\n        print(\"=\" * 80)\n        \n        # DOF Analysis\n        print(\"\\nüìÑ DOF - Requerimientos de Parsing:\")\n        if self.analysis['DOF']['samples']:\n            sample = self.analysis['DOF']['samples'][0]\n            objeto = sample.get('objeto_licitacion', '')\n            \n            print(f\"  üìù Ejemplo de objeto_licitacion:\")\n            print(f\"     {objeto[:200]}...\" if len(objeto) > 200 else f\"     {objeto}\")\n            \n            # Detectar qu√© necesita parsing\n            parsing_needed = []\n            if 'volumen' in objeto.lower():\n                parsing_needed.append(\"‚úÖ Volumen/cantidad\")\n            if any(fecha in objeto.lower() for fecha in ['agosto', 'septiembre', 'de 2025']):\n                parsing_needed.append(\"‚úÖ Fechas en texto\")\n            if re.search(r'\\d{1,2}:\\d{2}\\s*horas?', objeto, re.IGNORECASE):\n                parsing_needed.append(\"‚úÖ Horarios\")\n            if any(lugar in objeto.upper() for lugar in ['CIUDAD', 'MEXICO', 'ESTADO']):\n                parsing_needed.append(\"‚úÖ Ubicaciones\")\n            \n            print(\"  üîß Parsing requerido:\")\n            for req in parsing_needed:\n                print(f\"     {req}\")\n            \n            if not parsing_needed:\n                print(\"     ‚ùì No se detectaron patrones obvios de parsing\")\n        \n        # ComprasMX Analysis\n        print(\"\\nüè¢ ComprasMX - Requerimientos de Parsing:\")\n        if self.analysis['ComprasMX']['samples']:\n            sample = self.analysis['ComprasMX']['samples'][0]\n            \n            structured_fields = [\n                'numero_procedimiento', 'nombre_procedimiento', 'tipo_procedimiento',\n                'fecha_aclaraciones', 'fecha_apertura', 'unidad_compradora'\n            ]\n            \n            print(\"  üìä Campos ya estructurados:\")\n            for field in structured_fields:\n                if field in sample:\n                    valor = sample[field]\n                    print(f\"     ‚úÖ {field}: {str(valor)[:50]}{'...' if len(str(valor)) > 50 else ''}\")\n            \n            print(\"  üîß Parsing requerido: ‚ùå M√çNIMO - Datos ya estructurados\")\n        \n        # Tianguis Digital Analysis\n        print(\"\\nüèõÔ∏è Tianguis Digital - Requerimientos de Parsing:\")\n        if self.analysis['TianguisDigital']['samples']:\n            sample = self.analysis['TianguisDigital']['samples'][0]\n            \n            # Campos complejos que podr√≠an necesitar parsing\n            complex_fields = [\n                'tender/title', 'tender/description', 'planning/rationale',\n                'tender/items/description'\n            ]\n            \n            print(\"  üìä Campos potencialmente complejos:\")\n            for field in complex_fields:\n                if field in sample and sample[field]:\n                    valor = sample[field]\n                    print(f\"     üìù {field}: {str(valor)[:100]}{'...' if len(str(valor)) > 100 else ''}\")\n            \n            print(\"  üîß Parsing requerido: ‚ö†Ô∏è MEDIO - Algunos campos pueden necesitar limpieza\")\n    \n    def generate_recommendations(self):\n        \"\"\"Generar recomendaciones basadas en el an√°lisis real.\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"üí° RECOMENDACIONES BASADAS EN DATOS REALES\")\n        print(\"=\" * 80)\n        \n        recomendaciones = [\n            \"\\nüéØ ESTRATEGIA DE PARSING DIFERENCIADA:\",\n            \"\",\n            \"üìÑ DOF (Parsing INTENSIVO):\",\n            \"  - ‚úÖ Implementar parser avanzado como el desarrollado\",\n            \"  - ‚úÖ Extraer fechas m√∫ltiples del campo 'objeto_licitacion'\",\n            \"  - ‚úÖ Separar t√≠tulo de descripci√≥n concatenada\",\n            \"  - ‚úÖ Extraer vol√∫menes, cantidades y ubicaciones del texto\",\n            \"  - ‚úÖ Normalizar horarios y fechas a formatos est√°ndar\",\n            \"\",\n            \"üè¢ ComprasMX (Parsing M√çNIMO):\",\n            \"  - ‚úÖ Usar campos ya estructurados directamente\",\n            \"  - ‚úÖ Mapear campos existentes a esquema com√∫n\",\n            \"  - ‚ö†Ô∏è Posible limpieza menor de nombres y descripciones\",\n            \"  - ‚úÖ Fechas ya en formato datetime est√°ndar\",\n            \"\",\n            \"üèõÔ∏è Tianguis Digital (Parsing MEDIO):\",\n            \"  - ‚úÖ Estructura OCDS est√°ndar, campos bien definidos\",\n            \"  - ‚ö†Ô∏è Algunos campos pueden estar en formato complejo\",\n            \"  - ‚úÖ Fechas probablemente en formato ISO\",\n            \"  - ‚ö†Ô∏è Revisar si los campos m√∫ltiples necesitan agregaci√≥n\",\n            \"\",\n            \"üìã MODELO DE BASE DE DATOS H√çBRIDO:\",\n            \"\",\n            \"‚úÖ CAMPOS UNIVERSALES (todas las fuentes):\",\n            \"  - titulo (con diferentes niveles de limpieza)\",\n            \"  - numero_procedimiento \",\n            \"  - entidad_compradora\",\n            \"  - fecha_publicacion (formatos variables)\",\n            \"\",\n            \"‚ö†Ô∏è CAMPOS ESPEC√çFICOS POR FUENTE:\",\n            \"  - DOF: fechas_parseadas (junta, apertura, fallo, visita)\",\n            \"  - DOF: ubicacion_extraida, volumen_extraido\",\n            \"  - ComprasMX: tipo_procedimiento, caracter, estatus\",\n            \"  - Tianguis: campos OCDS espec√≠ficos\",\n            \"\",\n            \"üîß IMPLEMENTACI√ìN SUGERIDA:\",\n            \"  1. Mantener tabla actual 'licitaciones' para compatibilidad\",\n            \"  2. Crear tabla 'licitaciones_enhanced' con campos parseados\",\n            \"  3. Implementar parsers espec√≠ficos por fuente:\",\n            \"     - DOFParser (clase desarrollada)\",\n            \"     - ComprasMXParser (mapeo directo)\",\n            \"     - TianguisParser (validaci√≥n y limpieza)\",\n            \"  4. JSONB para campos espec√≠ficos de cada fuente\",\n            \"\",\n            \"‚ö° PRIORIDAD DE DESARROLLO:\",\n            \"  1. üî• Implementar DOFParser (mayor beneficio)\",\n            \"  2. üî∂ Crear mappers para ComprasMX (f√°cil ganancia)\",\n            \"  3. üî∑ Analizar Tianguis con m√°s detalle\",\n            \"  4. üî∏ Crear esquema unificado final\"\n        ]\n        \n        for rec in recomendaciones:\n            print(rec)\n    \n    def run_analysis(self):\n        \"\"\"Ejecutar an√°lisis completo.\"\"\"\n        print(\"üîç INICIANDO AN√ÅLISIS REAL DE FUENTES DE DATOS\")\n        print(\"=\" * 80)\n        print(\"‚ùó BASADO √öNICAMENTE EN ARCHIVOS JSON/CSV EXISTENTES\")\n        print(\"‚ùó NO INVENTA DATOS - SOLO ANALIZA LO QUE EXISTE\")\n        \n        try:\n            self.analyze_dof_files()\n            self.analyze_comprasmx_files() \n            self.analyze_tianguis_files()\n            self.compare_sources()\n            self.analyze_parsing_requirements()\n            self.generate_recommendations()\n            \n        except Exception as e:\n            print(f\"‚ùå Error durante el an√°lisis: {e}\")\n            print(f\"Verifique que exista el directorio: {self.base_path}\")\n\ndef main():\n    analyzer = SourceDataAnalyzer()\n    analyzer.run_analysis()\n\nif __name__ == \"__main__\":\n    main()\n