#!/usr/bin/env python3
"""
Scraper ComprasMX corregido para capturar hash real y descripci√≥n completa
MODIFICADO: Capturar window.location.href para obtener hash UUID real de cada licitaci√≥n
MEJORADO: Extraer descripci√≥n detallada completa de cada licitaci√≥n individual
CORREGIDO: Eliminar l√≠mite artificial de 5 p√°ginas - procesar TODAS las p√°ginas disponibles
"""

import asyncio
import os
import re
import json
import hashlib
import time
from pathlib import Path
from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse
from playwright.async_api import async_playwright
from datetime import datetime
from typing import Dict, List, Set, Optional

# Configuraci√≥n de directorios
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent  # ../../../
SALIDA = project_root / "data" / "raw" / "comprasmx"
SALIDA.mkdir(parents=True, exist_ok=True)

print(f"[INFO] Scraper ComprasMX - Guardando archivos en: {SALIDA.absolute()}")

class ComprasMXScraper:
    def __init__(self, salida_dir: Path = SALIDA, max_paginas_procesar: int = None):
        self.salida_dir = salida_dir
        self.respuestas_capturadas = []
        self.urls_procesadas = set()
        self.expedientes_totales = []
        self.expedientes_ids = set()
        self.pagina_actual = 1
        self.total_paginas = None
        self.total_registros = None
        
        # Para extracci√≥n de detalles individuales con hash real
        self.detalles_extraidos = {}
        self.carpeta_detalles = self.salida_dir / "detalles"
        self.carpeta_detalles.mkdir(parents=True, exist_ok=True)
        self.extraer_detalles = True
        
        # NUEVO: Control de l√≠mite de p√°ginas (None = sin l√≠mite)
        self.max_paginas_procesar = max_paginas_procesar
        
    def nombre_archivo(self, url: str, content_type: str, incluir_timestamp: bool = True) -> Path:
        """Genera un nombre estable y legible por URL + query."""
        u = urlparse(url)
        base = u.path.strip("/").replace("/", "_")
        if not base:
            base = "root"
        
        if u.query:
            q = "&".join(sorted([f"{k}={v}" for k, v in parse_qsl(u.query)]))
            h = hashlib.sha1(q.encode("utf-8")).hexdigest()[:10]
            base = f"{base}_{h}"
        
        ext = ".json"
        if "pdf" in content_type:
            ext = ".pdf"
        elif "html" in content_type:
            ext = ".html"
        
        if incluir_timestamp:
            ts = time.strftime("%Y%m%d-%H%M%S")
            return self.salida_dir / f"{ts}_{base}{ext}"
        else:
            return self.salida_dir / f"{base}{ext}"
    
    async def capturar_respuesta(self, response):
        """Captura y analiza respuestas de la API."""
        url = response.url
        
        # Solo procesar URLs de expedientes
        if "whitney/sitiopublico/expedientes" not in url:
            return
        
        ctype = (response.headers or {}).get("content-type", "").lower()
        
        try:
            if "application/json" in ctype or "json" in ctype:
                data = await response.json()
                
                # Guardar respuesta
                ruta = self.nombre_archivo(url, ctype)
                with open(ruta, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                print(f"\n[OK] JSON guardado: {ruta.name}")
                
                # Procesar datos seg√∫n la estructura real
                if isinstance(data, dict) and "data" in data:
                    for item in data["data"]:
                        # Procesar registros (expedientes)
                        if "registros" in item and isinstance(item["registros"], list):
                            nuevos_expedientes = 0
                            for exp in item["registros"]:
                                # Usar cod_expediente como ID √∫nico
                                if "cod_expediente" in exp and exp["cod_expediente"] not in self.expedientes_ids:
                                    self.expedientes_ids.add(exp["cod_expediente"])
                                    self.expedientes_totales.append(exp)
                                    nuevos_expedientes += 1
                            
                            print(f"  ‚îî‚îÄ Nuevos expedientes en esta respuesta: {nuevos_expedientes}")
                            print(f"  ‚îî‚îÄ Total expedientes √∫nicos capturados: {len(self.expedientes_ids)}")
                        
                        # Extraer informaci√≥n de paginaci√≥n
                        if "paginacion" in item and isinstance(item["paginacion"], list):
                            for pag_info in item["paginacion"]:
                                self.pagina_actual = pag_info.get("pagina_actual", 1)
                                self.total_paginas = pag_info.get("total_paginas", 1)
                                self.total_registros = pag_info.get("total_registros", 0)
                                
                                print(f"  ‚îî‚îÄ Paginaci√≥n: P√°gina {self.pagina_actual}/{self.total_paginas}")
                                print(f"  ‚îî‚îÄ Total registros en el sistema: {self.total_registros}")
                
                # Guardar respuesta para an√°lisis
                self.respuestas_capturadas.append({
                    "url": url,
                    "data": data,
                    "timestamp": datetime.now().isoformat()
                })
                
        except Exception as e:
            print(f"[ERROR] No se pudo procesar {url}: {e}")
    
    async def procesar_licitaciones_en_pagina_actual(self, page):
        """CORREGIDO: Procesa cada licitaci√≥n individual usando selectores PrimeNG espec√≠ficos"""
        if not self.extraer_detalles:
            return
            
        print(f"\n=== EXTRAYENDO DETALLES CON HASH REAL - P√ÅGINA {self.pagina_actual} ===")
        
        try:
            # NUEVO: Esperar tabla PrimeNG espec√≠fica
            await page.wait_for_selector("table.p-datatable-table", timeout=15000)
            print(f"  ‚úÖ Tabla PrimeNG detectada")
            
            # NUEVO: Obtener filas usando selectores PrimeNG espec√≠ficos
            filas_tabla = await page.query_selector_all("table.p-datatable-table tbody tr")
            
            if not filas_tabla:
                print(f"  ‚ùå No se encontraron filas en la tabla PrimeNG")
                return
            
            print(f"  ‚îî‚îÄ Encontradas {len(filas_tabla)} filas de licitaciones")
            
        except Exception as e:
            print(f"  ‚ùå Error localizando tabla PrimeNG: {e}")
            return
        
        # Procesar cada fila individual - UNA POR UNA COMPLETAMENTE
        for i, fila in enumerate(filas_tabla, 1):
            try:
                # NUEVO: Extraer informaci√≥n usando selectores espec√≠ficos de columna
                segunda_columna = await fila.query_selector("td:nth-child(2)")
                tercera_columna = await fila.query_selector("td:nth-child(3)")
                cuarta_columna = await fila.query_selector("td:nth-child(4)")
                
                if not segunda_columna:
                    continue
                
                # Extraer c√≥digo de expediente de la segunda columna
                codigo_expediente = await segunda_columna.inner_text()
                codigo_expediente = codigo_expediente.strip()
                
                if not codigo_expediente or len(codigo_expediente) < 5:
                    continue
                
                # Extraer t√≠tulo de la tercera o cuarta columna
                titulo_preliminar = ""
                if cuarta_columna:
                    titulo_preliminar = await cuarta_columna.inner_text()
                elif tercera_columna:
                    titulo_preliminar = await tercera_columna.inner_text()
                
                titulo_preliminar = titulo_preliminar.strip()
                
                print(f"\n[{i}/{len(filas_tabla)}] Procesando: {codigo_expediente}")
                print(f"    ‚îî‚îÄ {titulo_preliminar[:60]}...")
                
                # Verificar si ya procesamos este detalle
                if codigo_expediente in self.detalles_extraidos:
                    print(f"    ‚úì Ya procesado: {codigo_expediente}")
                    continue
                
                # NUEVO: Click espec√≠fico en la segunda columna donde est√° el c√≥digo
                print(f"    üîÑ Haciendo click en c√≥digo de expediente...")
                
                # Buscar enlace espec√≠fico dentro de la segunda columna
                enlace_codigo = await segunda_columna.query_selector("a, span[role='button'], div[role='button']")
                
                if not enlace_codigo:
                    # Si no hay enlace espec√≠fico, hacer click en la celda completa
                    enlace_codigo = segunda_columna
                    print(f"    ‚ö†Ô∏è No se encontr√≥ enlace espec√≠fico, usando celda completa")
                else:
                    print(f"    ‚úì Encontrado enlace en c√≥digo de expediente")
                
                # Ejecutar click en el c√≥digo de expediente
                click_exitoso = False
                try:
                    # Hacer click con JavaScript (m√°s confiable para PrimeNG)
                    await enlace_codigo.click()
                    print(f"    üîÑ Click ejecutado, esperando navegaci√≥n...")
                    click_exitoso = True
                except Exception as e:
                    print(f"    ‚ùå Error en click directo: {e}")
                    
                    # M√©todo alternativo: usar JavaScript
                    try:
                        await page.evaluate("(element) => element.click()", enlace_codigo)
                        print(f"    üîÑ Click por JavaScript ejecutado")
                        click_exitoso = True
                    except:
                        print(f"    ‚ùå Todos los m√©todos de click fallaron")
                
                if not click_exitoso:
                    # Si no se pudo hacer click, guardar informaci√≥n b√°sica y continuar
                    print(f"    ‚ö†Ô∏è No se pudo hacer click - guardando informaci√≥n b√°sica")
                    detalle = {
                        "codigo_expediente": codigo_expediente,
                        "url_completa_con_hash": "https://comprasmx.buengobierno.gob.mx/sitiopublico/#/",
                        "hash_uuid_real": None,
                        "descripcion_completa": "",
                        "informacion_extraida": {"titulo": titulo_preliminar},
                        "timestamp_procesamiento": datetime.now().isoformat(),
                        "procesado_exitosamente": False,
                        "pagina_origen": self.pagina_actual,
                        "error": "No se pudo hacer click en la licitaci√≥n"
                    }
                    await self.guardar_detalle_individual(codigo_expediente, detalle)
                    self.detalles_extraidos[codigo_expediente] = detalle
                    continue
                
                print(f"    ‚è≥ Esperando navegaci√≥n...")
                
                # 2. ESPERAR QUE CAMBIE LA URL (m√°ximo 15 segundos)
                url_cambio = False
                try:
                    await page.wait_for_function(
                        "window.location.href.includes('/detalle/') && window.location.href.includes('/procedimiento')",
                        timeout=15000
                    )
                    url_cambio = True
                    print(f"    ‚úÖ URL cambi√≥ a p√°gina de detalle")
                except:
                    print(f"    ‚ùå URL NO CAMBI√ì - El click no naveg√≥ correctamente")
                
                # 3. SI LA URL CAMBI√ì, PROCESAR; SI NO, SALTAR
                if url_cambio:
                    # CORREGIDO: Esperar carga completa del detalle con timeouts m√°s largos
                    print(f"    ‚è≥ Esperando carga completa de la p√°gina de detalle...")
                    await page.wait_for_load_state("networkidle", timeout=20000)  # Aumentado a 20 segundos
                    await page.wait_for_timeout(8000)  # Aumentado a 8 segundos para cargas din√°micas
                    print(f"    ‚úÖ P√°gina de detalle completamente cargada")
                    
                    # CAPTURAR INFORMACI√ìN CON TIEMPO SUFICIENTE
                    url_completa_real = await page.evaluate("window.location.href")
                    print(f"    üåê URL capturada: {url_completa_real}")
                    
                    hash_uuid = self.extraer_hash_de_url(url_completa_real)
                    if hash_uuid:
                        print(f"    üîë Hash UUID: {hash_uuid}")
                    
                    print(f"    üìù Extrayendo descripci√≥n completa...")
                    descripcion_completa = await self.extraer_descripcion_completa(page)
                    print(f"    üìù Descripci√≥n extra√≠da: {len(descripcion_completa)} caracteres")
                    
                    print(f"    üìã Extrayendo informaci√≥n estructurada...")
                    informacion_extraida = await self.extraer_informacion_detalle_comprasmx(page)
                    
                    if hash_uuid:
                        informacion_extraida["hash_uuid_real"] = hash_uuid
                    if descripcion_completa:
                        informacion_extraida["descripcion_completa"] = descripcion_completa
                    
                    # Guardar detalle
                    detalle = {
                        "codigo_expediente": codigo_expediente,
                        "url_completa_con_hash": url_completa_real,
                        "hash_uuid_real": hash_uuid,
                        "descripcion_completa": descripcion_completa,
                        "informacion_extraida": informacion_extraida,
                        "timestamp_procesamiento": datetime.now().isoformat(),
                        "procesado_exitosamente": True,
                        "pagina_origen": self.pagina_actual
                    }
                    
                    await self.guardar_detalle_individual(codigo_expediente, detalle)
                    self.detalles_extraidos[codigo_expediente] = detalle
                    print(f"    ‚úÖ Detalle completo guardado: {codigo_expediente}")
                    
                    # 4. VOLVER AL LISTADO CON TIEMPO SUFICIENTE
                    print(f"    ‚¨ÖÔ∏è Volviendo al listado...")
                    await page.go_back()
                    await page.wait_for_load_state("networkidle", timeout=15000)  # Tiempo suficiente para volver
                    await page.wait_for_timeout(3000)  # Pausa extra para estabilizar
                    print(f"    ‚úÖ De vuelta en el listado")
                    
                else:
                    # El click no funcion√≥, crear detalle b√°sico sin informaci√≥n adicional
                    print(f"    ‚ö†Ô∏è No se pudo acceder al detalle - guardando informaci√≥n b√°sica")
                    detalle = {
                        "codigo_expediente": codigo_expediente,
                        "url_completa_con_hash": "https://comprasmx.buengobierno.gob.mx/sitiopublico/#/",
                        "hash_uuid_real": None,
                        "descripcion_completa": "",
                        "informacion_extraida": {},
                        "timestamp_procesamiento": datetime.now().isoformat(),
                        "procesado_exitosamente": False,
                        "pagina_origen": self.pagina_actual,
                        "error": "No se pudo hacer click en la licitaci√≥n"
                    }
                    
                    await self.guardar_detalle_individual(codigo_expediente, detalle)
                    self.detalles_extraidos[codigo_expediente] = detalle
                
            except Exception as e:
                print(f"    ‚ùå Error procesando licitaci√≥n {i}: {e}")
                
                # Recuperaci√≥n: volver al listado si es posible
                try:
                    current_url = await page.evaluate("window.location.href")
                    if "/detalle/" in current_url:
                        print(f"    üîÑ Intentando volver al listado...")
                        await page.go_back()
                        await page.wait_for_load_state("networkidle")
                        await page.wait_for_timeout(2000)
                except:
                    # Si todo falla, navegar directamente al listado
                    print(f"    üîÑ Navegando directamente al listado...")
                    await page.goto(
                        "https://comprasmx.buengobierno.gob.mx/sitiopublico/#/",
                        wait_until="domcontentloaded"
                    )
                    await page.wait_for_timeout(5000)
                
        print(f"\n‚úÖ P√°gina {self.pagina_actual} completada. Detalles extra√≠dos: {len(self.detalles_extraidos)}")
    
    async def navegar_todas_las_paginas(self, page):
        """üöÄ CORREGIDO: Navega por las p√°ginas seg√∫n l√≠mite especificado"""
        # Determinar cu√°ntas p√°ginas procesar ANTES de mostrar mensajes
        if self.total_paginas and self.total_paginas > 1:
            paginas_a_procesar = self.max_paginas_procesar or self.total_paginas
            limite_real = min(paginas_a_procesar, self.total_paginas)
            
            if self.max_paginas_procesar:
                print(f"\n=== PROCESANDO {limite_real} P√ÅGINA(S) - L√çMITE: {self.max_paginas_procesar} ===")
            else:
                print(f"\n=== PROCESANDO TODAS LAS {limite_real} P√ÅGINAS (SIN L√çMITE) ===")
                
            print(f"üìä Total p√°ginas detectadas: {self.total_paginas}")
            print(f"üìä Total registros en sistema: {self.total_registros}")
            print(f"üéØ P√°ginas que se procesar√°n: {limite_real}")
        else:
            print(f"\n=== PROCESANDO 1 P√ÅGINA - P√ÅGINA √öNICA ===")
        
        # Esperar a que se cargue la primera p√°gina
        await page.wait_for_timeout(5000)
        
        # Procesar licitaciones de la primera p√°gina
        if self.extraer_detalles:
            await self.procesar_licitaciones_en_pagina_actual(page)
        
        # Navegar por p√°ginas adicionales SOLO si hay m√°s de 1 p√°gina Y no estamos limitando a 1
        if self.total_paginas and self.total_paginas > 1 and (not self.max_paginas_procesar or self.max_paginas_procesar > 1):
            paginas_a_procesar = self.max_paginas_procesar or self.total_paginas
            limite_paginas = min(paginas_a_procesar, self.total_paginas)
            
            for pagina in range(2, limite_paginas + 1):
                print(f"\n[Navegando a p√°gina {pagina}/{limite_paginas}]")
                
                # M√©todo de navegaci√≥n usando selectores PrimeNG
                exito = False
                try:
                    # Buscar bot√≥n "Siguiente" usando selector PrimeNG espec√≠fico
                    boton_siguiente = await page.query_selector("button.p-paginator-next:not(.p-disabled)")
                    if boton_siguiente:
                        await boton_siguiente.click()
                        await page.wait_for_load_state("networkidle")
                        await page.wait_for_timeout(5000)
                        print(f"  ‚úì Navegado con bot√≥n PrimeNG siguiente")
                        exito = True
                    else:
                        print(f"  ‚ö†Ô∏è Bot√≥n siguiente PrimeNG no disponible o deshabilitado")
                except Exception as e:
                    print(f"  ‚ùå Error con bot√≥n PrimeNG siguiente: {e}")
                
                # M√©todo alternativo: bot√≥n de p√°gina espec√≠fica
                if not exito:
                    try:
                        # Buscar bot√≥n de p√°gina espec√≠fica
                        boton_pagina = await page.query_selector(f"button.p-paginator-page[aria-label='Page {pagina}']")
                        if not boton_pagina:
                            # Fallback: buscar por texto
                            boton_pagina = await page.query_selector(f"button:has-text('{pagina}')")
                        
                        if boton_pagina:
                            await boton_pagina.click()
                            await page.wait_for_load_state("networkidle")
                            await page.wait_for_timeout(5000)
                            print(f"  ‚úì Navegado a p√°gina {pagina} con bot√≥n espec√≠fico")
                            exito = True
                    except Exception as e:
                        print(f"  ‚ùå Error con bot√≥n de p√°gina espec√≠fica: {e}")
                
                if not exito:
                    print(f"  ‚ùå No se pudo navegar a p√°gina {pagina}")
                    continue
                
                # Procesar licitaciones de esta p√°gina
                if self.extraer_detalles:
                    await self.procesar_licitaciones_en_pagina_actual(page)
                
                # Verificar progreso
                print(f"  ‚îî‚îÄ Detalles extra√≠dos hasta ahora: {len(self.detalles_extraidos)}")
                print(f"  ‚îî‚îÄ Expedientes totales hasta ahora: {len(self.expedientes_ids)}")
                
                # Pausa entre p√°ginas para no sobrecargar el servidor
                await page.wait_for_timeout(3000)
        else:
            print("\nüéØ Solo hay 1 p√°gina de resultados - procesamiento completo")
    
    async def cambiar_cantidad_resultados(self, page):
        """üöÄ MEJORADO: Maximizar resultados por p√°gina para capturar m√°s datos."""
        print("\n=== MAXIMIZANDO RESULTADOS POR P√ÅGINA ===")
        
        selectores = [
            "select",
            "[class*='per-page']",
            "[class*='page-size']",
            "[class*='items-per']",
            "[class*='pageSize']",
            "select[name*='size']",
            "select[name*='page']"
        ]
        
        for selector in selectores:
            try:
                elementos = await page.locator(selector).all()
                for elemento in elementos:
                    if await elemento.is_visible():
                        # Intentar seleccionar el valor m√°ximo
                        opciones = await elemento.locator("option").all()
                        valores = []
                        for opcion in opciones:
                            texto = await opcion.text_content()
                            if texto and texto.strip().isdigit():
                                valores.append(int(texto.strip()))
                        
                        if valores:
                            max_valor = max(valores)
                            if max_valor > 10:
                                print(f"  üéØ Maximizando a {max_valor} resultados por p√°gina")
                                await elemento.select_option(str(max_valor))
                                await page.wait_for_timeout(5000)
                                print(f"  ‚úÖ Configurado para mostrar {max_valor} resultados por p√°gina")
                                return True
            except:
                pass
        
        print("  ‚ö†Ô∏è No se pudo cambiar cantidad de resultados - usando valor por defecto")
        return False
    
    async def ejecutar(self, headless: bool = True, extraer_detalles: bool = True, max_paginas: int = None):
        """üöÄ MEJORADO: Ejecuta el scraper completo sin l√≠mites artificiales"""
        self.extraer_detalles = extraer_detalles
        self.max_paginas_procesar = max_paginas
        
        print(f"\n{'='*70}")
        print(f"SCRAPER COMPRASMX - CAPTURA COMPLETA SIN L√çMITES")
        print(f"Hora: {datetime.now()}")
        print(f"Modo: {'Headless' if headless else 'Visible'}")
        print(f"Extraer detalles con hash real: {'S√≠' if extraer_detalles else 'No'}")
        print(f"L√≠mite de p√°ginas: {'Sin l√≠mite' if not max_paginas else str(max_paginas)}")
        print(f"{'='*70}\n")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=headless,
                args=["--no-sandbox", "--disable-blink-features=AutomationControlled"]
            )
            
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                locale="es-MX",
                viewport={"width": 1920, "height": 1080}
            )
            
            page = await context.new_page()
            
            # Configurar interceptor de respuestas
            page.on("response", lambda r: asyncio.create_task(self.capturar_respuesta(r)))
            
            # 1. Cargar p√°gina principal
            print("[1/4] Abriendo ComprasMX...")
            await page.goto(
                "https://comprasmx.buengobierno.gob.mx/sitiopublico/#/",
                wait_until="domcontentloaded"
            )
            
            await page.wait_for_load_state("networkidle")
            await page.wait_for_timeout(10000)
            
            print(f"  ‚îî‚îÄ Primera carga: {len(self.expedientes_ids)} expedientes")
            
            # 2. Optimizar resultados por p√°gina
            print("\n[2/4] Maximizando resultados por p√°gina...")
            await self.cambiar_cantidad_resultados(page)
            
            # 3. Navegar por TODAS las p√°ginas extrayendo detalles
            print("\n[3/4] Navegando por TODAS las p√°ginas (sin l√≠mites)...")
            await self.navegar_todas_las_paginas(page)
            
            # 4. Guardar resultados
            print("\n[4/4] Guardando resultados...")
            await self.guardar_resultados()
            
            await browser.close()
            
            # Mostrar estad√≠sticas finales
            self.mostrar_estadisticas()
    
    async def guardar_resultados(self):
        """Guarda todos los expedientes capturados con hash real incluido"""
        # Guardar expedientes con informaci√≥n de hash real integrada
        for expediente in self.expedientes_totales:
            codigo = expediente.get("cod_expediente")
            if codigo in self.detalles_extraidos:
                detalle = self.detalles_extraidos[codigo]
                # Integrar hash real en el expediente
                expediente["hash_uuid_real"] = detalle.get("hash_uuid_real")
                expediente["url_completa_con_hash"] = detalle.get("url_completa_con_hash")
                expediente["descripcion_completa"] = detalle.get("descripcion_completa")
        
        # Guardar todos los expedientes con datos enriquecidos
        archivo_expedientes = self.salida_dir / f"todos_expedientes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(archivo_expedientes, "w", encoding="utf-8") as f:
            json.dump({
                "fecha_captura": datetime.now().isoformat(),
                "total_expedientes": len(self.expedientes_totales),
                "expedientes_con_hash_real": len([e for e in self.expedientes_totales if e.get("hash_uuid_real")]),
                "expedientes": self.expedientes_totales
            }, f, ensure_ascii=False, indent=2)
        print(f"  ‚îî‚îÄ Expedientes con hash real guardados en: {archivo_expedientes.name}")
        
        # Guardar resumen completo
        resumen = {
            "fecha_captura": datetime.now().isoformat(),
            "total_expedientes_capturados": len(self.expedientes_ids),
            "total_registros_sistema": self.total_registros,
            "detalles_extraidos": len(self.detalles_extraidos),
            "expedientes_con_hash_real": len([e for e in self.expedientes_totales if e.get("hash_uuid_real")]),
            "porcentaje_hash_real": (len([e for e in self.expedientes_totales if e.get("hash_uuid_real")]) / len(self.expedientes_totales) * 100) if self.expedientes_totales else 0,
            "paginas_procesadas": self.pagina_actual,
            "cobertura_sistema": (len(self.expedientes_ids) / self.total_registros * 100) if self.total_registros else 0,
            "codigos_expedientes": sorted(list(self.expedientes_ids))
        }
        
        archivo_resumen = self.salida_dir / f"resumen_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(archivo_resumen, "w", encoding="utf-8") as f:
            json.dump(resumen, f, ensure_ascii=False, indent=2)
        print(f"  ‚îî‚îÄ Resumen guardado en: {archivo_resumen.name}")
        
        # √çndice de detalles con hash real
        if self.detalles_extraidos:
            indice_detalles = {
                "fecha_creacion": datetime.now().isoformat(),
                "total_detalles": len(self.detalles_extraidos),
                "detalles_con_hash": len([d for d in self.detalles_extraidos.values() if d.get("hash_uuid_real")]),
                "detalles": {
                    codigo: {
                        "archivo": f"detalle_{re.sub(r'[^\w\-_.]', '_', codigo)}.json",
                        "url_completa": detalle.get("url_completa_con_hash", ""),
                        "hash_uuid_real": detalle.get("hash_uuid_real", ""),
                        "descripcion_longitud": len(detalle.get("descripcion_completa", "")),
                        "procesado_en": detalle.get("timestamp_procesamiento", "")
                    }
                    for codigo, detalle in self.detalles_extraidos.items()
                }
            }
            
            archivo_indice = self.carpeta_detalles / "indice_detalles.json"
            with open(archivo_indice, "w", encoding="utf-8") as f:
                json.dump(indice_detalles, f, ensure_ascii=False, indent=2)
            print(f"  ‚îî‚îÄ √çndice de detalles con hash real guardado en: {archivo_indice.name}")
    
    def mostrar_estadisticas(self):
        """üöÄ MEJORADO: Muestra estad√≠sticas completas de captura"""
        print(f"\n{'='*70}")
        print("ESTAD√çSTICAS FINALES - CAPTURA COMPLETA")
        print(f"{'='*70}")
        print(f"‚úì Expedientes capturados: {len(self.expedientes_ids):,}")
        print(f"‚úì Total en el sistema: {self.total_registros:,}")
        print(f"‚úì P√°ginas procesadas: {self.pagina_actual}")
        if self.total_registros:
            cobertura = (len(self.expedientes_ids) / self.total_registros * 100)
            print(f"‚úì Cobertura del sistema: {cobertura:.1f}%")
            if cobertura < 90:
                print(f"‚ö†Ô∏è COBERTURA INCOMPLETA - Considera aumentar l√≠mite de p√°ginas")
        
        # Estad√≠sticas de hash real
        if self.extraer_detalles:
            expedientes_con_hash = len([e for e in self.expedientes_totales if e.get("hash_uuid_real")])
            print(f"‚úì Detalles individuales extra√≠dos: {len(self.detalles_extraidos):,}")
            print(f"‚úì Expedientes con hash UUID real: {expedientes_con_hash:,}")
            if self.expedientes_totales:
                porcentaje_hash = (expedientes_con_hash / len(self.expedientes_totales) * 100)
                print(f"‚úì Cobertura hash real: {porcentaje_hash:.1f}%")
            
            # Mostrar algunos ejemplos de hash capturados
            ejemplos_hash = [e.get("hash_uuid_real") for e in self.expedientes_totales if e.get("hash_uuid_real")]
            if ejemplos_hash:
                print(f"‚úì Ejemplos de hash UUID capturados:")
                for i, hash_ejemplo in enumerate(ejemplos_hash[:3], 1):
                    print(f"  {i}. {hash_ejemplo}")
        
        print(f"‚úì Archivos guardados en: {self.salida_dir.absolute()}")
        
        # Recomendaci√≥n final
        if self.total_registros and len(self.expedientes_ids) < self.total_registros:
            faltantes = self.total_registros - len(self.expedientes_ids)
            print(f"\nüìã RECOMENDACI√ìN:")
            print(f"   - Faltan {faltantes:,} expedientes por capturar")
            print(f"   - Ejecutar sin l√≠mite de p√°ginas para captura completa")
        
        print(f"\n{'='*70}")
        print(f"CAPTURA COMPLETADA: {datetime.now()}")
        print(f"{'='*70}\n")


async def main():
    """Funci√≥n principal mejorada con opciones flexibles"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Scraper ComprasMX sin l√≠mites')
    parser.add_argument('--headless', action='store_true', help='Ejecutar en modo headless')
    parser.add_argument('--no-detalles', action='store_true', help='No extraer detalles individuales')
    parser.add_argument('--max-paginas', type=int, help='L√≠mite m√°ximo de p√°ginas a procesar')
    
    args = parser.parse_args()
    
    scraper = ComprasMXScraper()
    
    # Ejecutar con par√°metros especificados
    await scraper.ejecutar(
        headless=args.headless,
        extraer_detalles=not args.no_detalles,
        max_paginas=args.max_paginas
    )


if __name__ == "__main__":
    asyncio.run(main())