#!/usr/bin/env python3
"""
Scraper ComprasMX corregido para capturar hash real y descripci√≥n completa
MODIFICADO: Capturar window.location.href para obtener hash UUID real de cada licitaci√≥n
MEJORADO: Extraer descripci√≥n detallada completa de cada licitaci√≥n individual
"""

import asyncio
import os
import re
import json
import hashlib
import time
from pathlib import Path
from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse
from playwright.async_api import async_playwright
from datetime import datetime
from typing import Dict, List, Set, Optional

# Configuraci√≥n de directorios
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent  # ../../../
SALIDA = project_root / "data" / "raw" / "comprasmx"
SALIDA.mkdir(parents=True, exist_ok=True)

print(f"[INFO] Scraper ComprasMX - Guardando archivos en: {SALIDA.absolute()}")

class ComprasMXScraper:
    def __init__(self, salida_dir: Path = SALIDA):
        self.salida_dir = salida_dir
        self.respuestas_capturadas = []
        self.urls_procesadas = set()
        self.expedientes_totales = []
        self.expedientes_ids = set()
        self.pagina_actual = 1
        self.total_paginas = None
        self.total_registros = None
        
        # Para extracci√≥n de detalles individuales con hash real
        self.detalles_extraidos = {}
        self.carpeta_detalles = self.salida_dir / "detalles"
        self.carpeta_detalles.mkdir(parents=True, exist_ok=True)
        self.extraer_detalles = True
        
    def nombre_archivo(self, url: str, content_type: str, incluir_timestamp: bool = True) -> Path:
        """Genera un nombre estable y legible por URL + query."""
        u = urlparse(url)
        base = u.path.strip("/").replace("/", "_")
        if not base:
            base = "root"
        
        if u.query:
            q = "&".join(sorted([f"{k}={v}" for k, v in parse_qsl(u.query)]))
            h = hashlib.sha1(q.encode("utf-8")).hexdigest()[:10]
            base = f"{base}_{h}"
        
        ext = ".json"
        if "pdf" in content_type:
            ext = ".pdf"
        elif "html" in content_type:
            ext = ".html"
        
        if incluir_timestamp:
            ts = time.strftime("%Y%m%d-%H%M%S")
            return self.salida_dir / f"{ts}_{base}{ext}"
        else:
            return self.salida_dir / f"{base}{ext}"
    
    async def capturar_respuesta(self, response):
        """Captura y analiza respuestas de la API."""
        url = response.url
        
        # Solo procesar URLs de expedientes
        if "whitney/sitiopublico/expedientes" not in url:
            return
        
        ctype = (response.headers or {}).get("content-type", "").lower()
        
        try:
            if "application/json" in ctype or "json" in ctype:
                data = await response.json()
                
                # Guardar respuesta
                ruta = self.nombre_archivo(url, ctype)
                with open(ruta, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                print(f"\\n[OK] JSON guardado: {ruta.name}")
                
                # Procesar datos seg√∫n la estructura real
                if isinstance(data, dict) and "data" in data:
                    for item in data["data"]:
                        # Procesar registros (expedientes)
                        if "registros" in item and isinstance(item["registros"], list):
                            nuevos_expedientes = 0
                            for exp in item["registros"]:
                                # Usar cod_expediente como ID √∫nico
                                if "cod_expediente" in exp and exp["cod_expediente"] not in self.expedientes_ids:
                                    self.expedientes_ids.add(exp["cod_expediente"])
                                    self.expedientes_totales.append(exp)
                                    nuevos_expedientes += 1
                            
                            print(f"  ‚îî‚îÄ Nuevos expedientes en esta respuesta: {nuevos_expedientes}")
                            print(f"  ‚îî‚îÄ Total expedientes √∫nicos capturados: {len(self.expedientes_ids)}")
                        
                        # Extraer informaci√≥n de paginaci√≥n
                        if "paginacion" in item and isinstance(item["paginacion"], list):
                            for pag_info in item["paginacion"]:
                                self.pagina_actual = pag_info.get("pagina_actual", 1)
                                self.total_paginas = pag_info.get("total_paginas", 1)
                                self.total_registros = pag_info.get("total_registros", 0)
                                
                                print(f"  ‚îî‚îÄ Paginaci√≥n: P√°gina {self.pagina_actual}/{self.total_paginas}")
                                print(f"  ‚îî‚îÄ Total registros en el sistema: {self.total_registros}")
                
                # Guardar respuesta para an√°lisis
                self.respuestas_capturadas.append({
                    "url": url,
                    "data": data,
                    "timestamp": datetime.now().isoformat()
                })
                
        except Exception as e:
            print(f"[ERROR] No se pudo procesar {url}: {e}")
    
    async def procesar_licitaciones_en_pagina_actual(self, page):
        """CORREGIDO: Procesa cada licitaci√≥n individual capturando hash real y descripci√≥n completa"""
        if not self.extraer_detalles:
            return
            
        print(f"\\n=== EXTRAYENDO DETALLES CON HASH REAL - P√ÅGINA {self.pagina_actual} ===")
        
        try:
            # Esperar a que la tabla se cargue
            await page.wait_for_selector("table", timeout=15000)
            
            # Obtener todas las filas de la tabla (excluyendo header)
            filas_tabla = await page.locator("table tbody tr, table tr:not(:first-child)").all()
            
            if not filas_tabla:
                # Fallback: buscar filas con contenido relevante
                filas_tabla = await page.locator("tr").all()
                filas_filtradas = []
                for fila in filas_tabla:
                    texto = await fila.text_content()
                    # Filtrar headers y filas vac√≠as
                    if texto and not ("N√∫m." in texto and "N√∫mero de identificaci√≥n" in texto) and len(texto.strip()) > 20:
                        filas_filtradas.append(fila)
                filas_tabla = filas_filtradas
            
            print(f"  ‚îî‚îÄ Encontradas {len(filas_tabla)} filas de licitaciones")
            
        except Exception as e:
            print(f"  ‚ùå Error localizando tabla: {e}")
            return
        
        # Procesar cada fila individual
        for i, fila in enumerate(filas_tabla, 1):
            try:
                # Extraer informaci√≥n b√°sica de la fila
                celdas = await fila.locator("td").all()
                if len(celdas) < 3:
                    continue
                
                # Extraer c√≥digo de expediente (segunda columna)
                codigo_expediente = (await celdas[1].text_content()).strip()
                if not codigo_expediente or len(codigo_expediente) < 5:
                    continue
                
                # Extraer t√≠tulo preliminar (tercera o cuarta columna)
                titulo_preliminar = ""
                if len(celdas) >= 4:
                    titulo_preliminar = (await celdas[3].text_content()).strip()
                elif len(celdas) >= 3:
                    titulo_preliminar = (await celdas[2].text_content()).strip()
                
                print(f"\\n[{i}/{len(filas_tabla)}] Procesando: {codigo_expediente}")
                print(f"    ‚îî‚îÄ {titulo_preliminar[:60]}...")
                
                # Verificar si ya procesamos este detalle
                if codigo_expediente in self.detalles_extraidos:
                    print(f"    ‚úì Ya procesado: {codigo_expediente}")
                    continue
                
                # PASO CLAVE: Hacer click y navegar al detalle
                await fila.click()
                await page.wait_for_load_state("networkidle")
                await page.wait_for_timeout(5000)  # Tiempo extra para carga completa
                
                # CAPTURAR HASH REAL DE LA URL usando window.location.href
                url_completa_real = await page.evaluate("window.location.href")
                print(f"    üåê URL completa capturada: {url_completa_real}")
                
                # Extraer hash UUID de la URL
                hash_uuid = self.extraer_hash_de_url(url_completa_real)
                if hash_uuid:
                    print(f"    üîë Hash UUID extra√≠do: {hash_uuid}")
                else:
                    print(f"    ‚ö†Ô∏è No se pudo extraer hash UUID de la URL")
                
                # EXTRAER DESCRIPCI√ìN DETALLADA COMPLETA
                descripcion_completa = await self.extraer_descripcion_completa(page)
                print(f"    üìù Descripci√≥n extra√≠da: {len(descripcion_completa)} caracteres")
                
                # Extraer informaci√≥n estructurada completa
                informacion_extraida = await self.extraer_informacion_detalle_comprasmx(page)
                
                # INTEGRAR HASH REAL EN LA INFORMACI√ìN
                if hash_uuid:
                    informacion_extraida["hash_uuid_real"] = hash_uuid
                
                if descripcion_completa:
                    informacion_extraida["descripcion_completa"] = descripcion_completa
                
                # Crear objeto de detalle con datos reales
                detalle = {
                    "codigo_expediente": codigo_expediente,
                    "url_completa_con_hash": url_completa_real,
                    "hash_uuid_real": hash_uuid,  # NUEVO: Hash real extra√≠do
                    "descripcion_completa": descripcion_completa,  # NUEVO: Descripci√≥n completa
                    "informacion_extraida": informacion_extraida,
                    "timestamp_procesamiento": datetime.now().isoformat(),
                    "procesado_exitosamente": True,
                    "pagina_origen": self.pagina_actual
                }
                
                # Guardar detalle individual
                await self.guardar_detalle_individual(codigo_expediente, detalle)
                self.detalles_extraidos[codigo_expediente] = detalle
                
                print(f"    ‚úÖ Detalle completo guardado: {codigo_expediente}")
                
                # Volver al listado
                await page.go_back()
                await page.wait_for_load_state("networkidle")
                await page.wait_for_timeout(3000)
                
            except Exception as e:
                print(f"    ‚ùå Error procesando licitaci√≥n {i}: {e}")
                
                # Intentar recuperaci√≥n volviendo al listado
                try:
                    await page.go_back()
                    await page.wait_for_timeout(3000)
                except:
                    # Si falla, navegar directamente al inicio
                    await page.goto(
                        "https://comprasmx.buengobierno.gob.mx/sitiopublico/#/",
                        wait_until="domcontentloaded"
                    )
                    await page.wait_for_timeout(5000)
                
        print(f"\\n‚úÖ P√°gina {self.pagina_actual} completada. Detalles extra√≠dos: {len(self.detalles_extraidos)}")
    
    def extraer_hash_de_url(self, url_completa: str) -> str:
        """NUEVA FUNCI√ìN: Extrae el hash UUID de la URL de ComprasMX"""
        try:
            # Patr√≥n para URLs de ComprasMX con hash
            # Ejemplo: https://comprasmx.buengobierno.gob.mx/sitiopublico/#/sitiopublico/detalle/4b30105081ee4ce5b44aea1bf6eac6dc/procedimiento
            patron = r'\/detalle\/([a-f0-9]{32})\/procedimiento'
            
            match = re.search(patron, url_completa)
            if match:
                return match.group(1)
            
            # Patr√≥n alternativo m√°s general
            patron_alt = r'\/([a-f0-9]{32})\/'
            match_alt = re.search(patron_alt, url_completa)
            if match_alt:
                return match_alt.group(1)
                
            return None
            
        except Exception as e:
            print(f"    ‚ùå Error extrayendo hash: {e}")
            return None
    
    async def extraer_descripcion_completa(self, page) -> str:
        """NUEVA FUNCI√ìN: Extrae la descripci√≥n detallada completa de la p√°gina"""
        try:
            # Buscar espec√≠ficamente el campo de descripci√≥n detallada
            selectores_descripcion = [
                # Selector espec√≠fico para el campo de descripci√≥n detallada
                "[contains(text(), 'Descripci√≥n detallada del procedimiento')]//following-sibling::*[1]",
                # Selector por texto visible
                "xpath=//text()[contains(., 'Descripci√≥n detallada')]/following::text()[1]",
                # Selectores de respaldo
                ".descripcion-detallada",
                "[data-field='descripcion_detallada']",
                ".detalle-descripcion"
            ]
            
            # Tambi√©n buscar por patrones de texto en el HTML
            contenido_html = await page.content()
            
            # Patr√≥n para encontrar descripci√≥n detallada
            patron_descripcion = r'Descripci√≥n detallada del procedimiento de contrataci√≥n:\s*([^\n]+(?:\n[^\n]+)*?)(?:\nLey/Soporte|$)'
            
            match = re.search(patron_descripcion, contenido_html, re.MULTILINE | re.DOTALL)
            if match:
                descripcion = match.group(1).strip()
                # Limpiar HTML tags si existen
                descripcion = re.sub(r'<[^>]+>', '', descripcion)
                # Limpiar espacios extra
                descripcion = re.sub(r'\s+', ' ', descripcion).strip()
                return descripcion
            
            # M√©todo alternativo: buscar por selectores CSS
            for selector in selectores_descripcion:
                try:
                    if not selector.startswith("xpath="):
                        elemento = page.locator(selector).first
                        if await elemento.is_visible():
                            texto = await elemento.text_content()
                            if texto and len(texto.strip()) > 20:
                                return texto.strip()
                except:
                    continue
            
            return ""
            
        except Exception as e:
            print(f"    ‚ùå Error extrayendo descripci√≥n completa: {e}")
            return ""
    
    async def extraer_informacion_detalle_comprasmx(self, page) -> Dict:
        """Extrae informaci√≥n estructurada espec√≠fica de ComprasMX con hash real incluido"""
        try:
            informacion = {
                "codigo_expediente": "",
                "numero_procedimiento": "",
                "estatus": "",
                "dependencia_entidad": "",
                "unidad_compradora": "",
                "responsable_captura": "",
                "email_unidad_compradora": "",
                "descripcion_detallada": "",
                "tipo_procedimiento": "",
                "entidad_federativa": "",
                "a√±o_ejercicio": "",
                "fechas_cronograma": {},
                "partidas_especificas": [],
                "datos_especificos": {},
                "documentos_anexos": [],
                "requisitos_economicos": []
            }
            
            # Obtener todo el contenido HTML
            contenido = await page.content()
            
            # Extraer campos usando patrones mejorados
            campos_patrones = [
                ("codigo_expediente", r'C√≥digo del expediente:\s*([^\n]+)'),
                ("numero_procedimiento", r'N√∫mero de procedimiento de contrataci√≥n:\s*([^\n]+)'),
                ("estatus", r'Estatus del procedimiento de contrataci√≥n:\s*([^\n]+)'),
                ("dependencia_entidad", r'Dependencia o Entidad:\s*([^\n]+)'),
                ("unidad_compradora", r'Unidad compradora:\s*([^\n]+)'),
                ("responsable_captura", r'Responsable de la captura:\s*([^\n]+)'),
                ("email_unidad_compradora", r'Correo electr√≥nico unidad compradora:\s*([^\n]+)'),
                ("descripcion_detallada", r'Descripci√≥n detallada del procedimiento de contrataci√≥n:\s*([^\n]+(?:\n[^\n]+)*?)(?:\nLey|$)'),
                ("tipo_procedimiento", r'Tipo de procedimiento de contrataci√≥n:\s*([^\n]+)'),
                ("entidad_federativa", r'Entidad Federativa donde se llevar√° a cabo la contrataci√≥n:\s*([^\n]+)'),
                ("a√±o_ejercicio", r'A√±o del ejercicio presupuestal:\s*([^\n]+)')
            ]
            
            for campo, patron in campos_patrones:
                try:
                    if campo == "descripcion_detallada":
                        match = re.search(patron, contenido, re.MULTILINE | re.DOTALL)
                    else:
                        match = re.search(patron, contenido)
                    
                    if match:
                        valor = match.group(1).strip()
                        # Limpiar HTML tags
                        valor = re.sub(r'<[^>]+>', '', valor)
                        # Limpiar espacios extra
                        valor = re.sub(r'\s+', ' ', valor).strip()
                        informacion[campo] = valor
                except:
                    pass
            
            # FECHAS DEL CRONOGRAMA (formato estandarizado)
            try:
                fechas_campos = [
                    ("fecha_publicacion", r'Fecha y hora de publicaci√≥n:\s*([^\n]+)'),
                    ("fecha_apertura", r'Fecha y hora de presentaci√≥n y apertura de proposiciones:\s*([^\n]+)'),
                    ("fecha_junta_aclaraciones", r'Fecha y hora de junta de aclaraciones:\s*([^\n]+)'),
                    ("fecha_fallo", r'Fecha y hora del acto del Fallo:\s*([^\n]+)'),
                    ("fecha_inicio_estimada", r'Fecha estimada del inicio del contrato:\s*([^\n]+)'),
                    ("lugar_apertura", r'Lugar de apertura de proposiciones:\s*([^\n]+)'),
                    ("lugar_junta_aclaraciones", r'Lugar de la junta de aclaraciones:\s*([^\n]+)'),
                    ("lugar_fallo", r'Lugar del acto del Fallo:\s*([^\n]+)')
                ]
                
                for clave, pattern in fechas_campos:
                    match = re.search(pattern, contenido)
                    if match:
                        valor = match.group(1).strip()
                        informacion["fechas_cronograma"][clave] = valor
            except:
                pass
            
            return informacion
            
        except Exception as e:
            print(f"    ‚ùå Error en extracci√≥n de detalle: {e}")
            return {}
    
    async def guardar_detalle_individual(self, codigo_expediente: str, detalle: Dict):
        """Guarda el detalle individual con hash real en archivo JSON"""
        try:
            # Limpiar c√≥digo de expediente para nombre de archivo
            codigo_limpio = re.sub(r'[^\\w\\-_.]', '_', codigo_expediente)
            archivo_detalle = self.carpeta_detalles / f"detalle_{codigo_limpio}.json"
            
            with open(archivo_detalle, "w", encoding="utf-8") as f:
                json.dump(detalle, f, ensure_ascii=False, indent=2)
                
            print(f"    üíæ Detalle guardado: {archivo_detalle.name}")
                
        except Exception as e:
            print(f"    ‚ùå Error guardando detalle {codigo_expediente}: {e}")
    
    async def navegar_todas_las_paginas(self, page):
        """Navega por todas las p√°ginas procesando detalles en cada una"""
        print("\\n=== NAVEGANDO POR TODAS LAS P√ÅGINAS CON EXTRACCI√ìN DE DETALLES ===")
        
        # Esperar a que se cargue la primera p√°gina
        await page.wait_for_timeout(5000)
        
        # Procesar licitaciones de la primera p√°gina
        if self.extraer_detalles:
            await self.procesar_licitaciones_en_pagina_actual(page)
        
        # Navegar por p√°ginas adicionales si existen
        if self.total_paginas and self.total_paginas > 1:
            print(f"\\nDetectadas {self.total_paginas} p√°ginas con {self.total_registros} registros totales")
            
            for pagina in range(2, min(self.total_paginas + 1, 6)):  # Limitar a 5 p√°ginas para prueba
                print(f"\\n[Navegando a p√°gina {pagina}/{self.total_paginas}]")
                
                # M√©todo de navegaci√≥n por botones de p√°gina
                exito = False
                
                try:
                    # Buscar y hacer click en bot√≥n de p√°gina espec√≠fica
                    boton_pagina = page.locator(f"button:has-text('{pagina}')").first
                    if await boton_pagina.is_visible():
                        await boton_pagina.click()
                        await page.wait_for_load_state("networkidle")
                        await page.wait_for_timeout(5000)
                        print(f"  ‚úì Navegado a p√°gina {pagina}")
                        exito = True
                except:
                    pass
                
                # M√©todo alternativo: bot√≥n "Siguiente"
                if not exito:
                    try:
                        boton_siguiente = page.locator("button:has-text('Siguiente'), button:has-text('>')").first
                        if await boton_siguiente.is_visible() and await boton_siguiente.is_enabled():
                            await boton_siguiente.click()
                            await page.wait_for_load_state("networkidle")
                            await page.wait_for_timeout(5000)
                            print(f"  ‚úì Navegado con bot√≥n siguiente")
                            exito = True
                    except:
                        pass
                
                if not exito:
                    print(f"  ‚ùå No se pudo navegar a p√°gina {pagina}")
                    continue
                
                # Procesar licitaciones de esta p√°gina
                if self.extraer_detalles:
                    await self.procesar_licitaciones_en_pagina_actual(page)
                
                # Verificar progreso
                print(f"  ‚îî‚îÄ Detalles extra√≠dos hasta ahora: {len(self.detalles_extraidos)}")
                
                # Pausa entre p√°ginas para no sobrecargar el servidor
                await page.wait_for_timeout(3000)
    
    async def cambiar_cantidad_resultados(self, page):
        """Intenta cambiar la cantidad de resultados por p√°gina al m√°ximo."""
        print("\\n=== CAMBIANDO CANTIDAD DE RESULTADOS ===")
        
        selectores = [
            "select",
            "[class*='per-page']",
            "[class*='page-size']",
            "[class*='items-per']"
        ]
        
        for selector in selectores:
            try:
                elementos = await page.locator(selector).all()
                for elemento in elementos:
                    if await elemento.is_visible():
                        # Intentar seleccionar el valor m√°ximo
                        opciones = await elemento.locator("option").all()
                        valores = []
                        for opcion in opciones:
                            texto = await opcion.text_content()
                            if texto and texto.strip().isdigit():
                                valores.append(int(texto.strip()))
                        
                        if valores:
                            max_valor = max(valores)
                            if max_valor > 10:
                                print(f"  ‚îî‚îÄ Cambiando a mostrar {max_valor} resultados por p√°gina")
                                await elemento.select_option(str(max_valor))
                                await page.wait_for_timeout(3000)
                                return True
            except:
                pass
        
        print("  ‚îî‚îÄ Usando valor por defecto de resultados por p√°gina")
        return False
    
    async def ejecutar(self, headless: bool = True, extraer_detalles: bool = True):
        """Ejecuta el scraper completo con extracci√≥n de hash real y descripci√≥n"""
        self.extraer_detalles = extraer_detalles
        
        print(f"\\n{'='*70}")
        print(f"SCRAPER COMPRASMX - CAPTURA HASH REAL + DESCRIPCI√ìN COMPLETA")
        print(f"Hora: {datetime.now()}")
        print(f"Modo: {'Headless' if headless else 'Visible'}")
        print(f"Extraer detalles con hash real: {'S√≠' if extraer_detalles else 'No'}")
        print(f"{'='*70}\\n")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=headless,
                args=["--no-sandbox", "--disable-blink-features=AutomationControlled"]
            )
            
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                locale="es-MX",
                viewport={"width": 1920, "height": 1080}
            )
            
            page = await context.new_page()
            
            # Configurar interceptor de respuestas
            page.on("response", lambda r: asyncio.create_task(self.capturar_respuesta(r)))
            
            # 1. Cargar p√°gina principal
            print("[1/4] Abriendo ComprasMX...")
            await page.goto(
                "https://comprasmx.buengobierno.gob.mx/sitiopublico/#/",
                wait_until="domcontentloaded"
            )
            
            await page.wait_for_load_state("networkidle")
            await page.wait_for_timeout(10000)
            
            print(f"  ‚îî‚îÄ Primera carga: {len(self.expedientes_ids)} expedientes")
            
            # 2. Optimizar resultados por p√°gina
            print("\\n[2/4] Optimizando cantidad de resultados...")
            await self.cambiar_cantidad_resultados(page)
            
            # 3. Navegar por p√°ginas extrayendo detalles
            print("\\n[3/4] Navegando y extrayendo detalles con hash real...")
            await self.navegar_todas_las_paginas(page)
            
            # 4. Guardar resultados
            print("\\n[4/4] Guardando resultados...")
            await self.guardar_resultados()
            
            await browser.close()
            
            # Mostrar estad√≠sticas finales
            self.mostrar_estadisticas()
    
    async def guardar_resultados(self):
        """Guarda todos los expedientes capturados con hash real incluido"""
        # Guardar expedientes con informaci√≥n de hash real integrada
        for expediente in self.expedientes_totales:
            codigo = expediente.get("cod_expediente")
            if codigo in self.detalles_extraidos:
                detalle = self.detalles_extraidos[codigo]
                # Integrar hash real en el expediente
                expediente["hash_uuid_real"] = detalle.get("hash_uuid_real")
                expediente["url_completa_con_hash"] = detalle.get("url_completa_con_hash")
                expediente["descripcion_completa"] = detalle.get("descripcion_completa")
        
        # Guardar todos los expedientes con datos enriquecidos
        archivo_expedientes = self.salida_dir / f"todos_expedientes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(archivo_expedientes, "w", encoding="utf-8") as f:
            json.dump({
                "fecha_captura": datetime.now().isoformat(),
                "total_expedientes": len(self.expedientes_totales),
                "expedientes_con_hash_real": len([e for e in self.expedientes_totales if e.get("hash_uuid_real")]),
                "expedientes": self.expedientes_totales
            }, f, ensure_ascii=False, indent=2)
        print(f"  ‚îî‚îÄ Expedientes con hash real guardados en: {archivo_expedientes.name}")
        
        # Guardar resumen completo
        resumen = {
            "fecha_captura": datetime.now().isoformat(),
            "total_expedientes_capturados": len(self.expedientes_ids),
            "total_registros_sistema": self.total_registros,
            "detalles_extraidos": len(self.detalles_extraidos),
            "expedientes_con_hash_real": len([e for e in self.expedientes_totales if e.get("hash_uuid_real")]),
            "porcentaje_hash_real": (len([e for e in self.expedientes_totales if e.get("hash_uuid_real")]) / len(self.expedientes_totales) * 100) if self.expedientes_totales else 0,
            "codigos_expedientes": sorted(list(self.expedientes_ids))
        }
        
        archivo_resumen = self.salida_dir / f"resumen_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(archivo_resumen, "w", encoding="utf-8") as f:
            json.dump(resumen, f, ensure_ascii=False, indent=2)
        print(f"  ‚îî‚îÄ Resumen guardado en: {archivo_resumen.name}")
        
        # √çndice de detalles con hash real
        if self.detalles_extraidos:
            indice_detalles = {
                "fecha_creacion": datetime.now().isoformat(),
                "total_detalles": len(self.detalles_extraidos),
                "detalles_con_hash": len([d for d in self.detalles_extraidos.values() if d.get("hash_uuid_real")]),
                "detalles": {
                    codigo: {
                        "archivo": f"detalle_{re.sub(r'[^\\\\w\\\\-_.]', '_', codigo)}.json",
                        "url_completa": detalle.get("url_completa_con_hash", ""),
                        "hash_uuid_real": detalle.get("hash_uuid_real", ""),
                        "descripcion_longitud": len(detalle.get("descripcion_completa", "")),
                        "procesado_en": detalle.get("timestamp_procesamiento", "")
                    }
                    for codigo, detalle in self.detalles_extraidos.items()
                }
            }
            
            archivo_indice = self.carpeta_detalles / "indice_detalles.json"
            with open(archivo_indice, "w", encoding="utf-8") as f:
                json.dump(indice_detalles, f, ensure_ascii=False, indent=2)
            print(f"  ‚îî‚îÄ √çndice de detalles con hash real guardado en: {archivo_indice.name}")
    
    def mostrar_estadisticas(self):
        """Muestra estad√≠sticas finales incluyendo hash real capturado"""
        print(f"\\n{'='*70}")
        print("ESTAD√çSTICAS FINALES - CAPTURA CON HASH REAL")
        print(f"{'='*70}")
        print(f"‚úì Expedientes capturados: {len(self.expedientes_ids)}")
        print(f"‚úì Total en el sistema: {self.total_registros}")
        if self.total_registros:
            porcentaje = (len(self.expedientes_ids) / self.total_registros * 100)
            print(f"‚úì Porcentaje capturado: {porcentaje:.1f}%")
        
        # Estad√≠sticas de hash real
        if self.extraer_detalles:
            expedientes_con_hash = len([e for e in self.expedientes_totales if e.get("hash_uuid_real")])
            print(f"‚úì Detalles individuales extra√≠dos: {len(self.detalles_extraidos)}")
            print(f"‚úì Expedientes con hash UUID real: {expedientes_con_hash}")
            if self.expedientes_totales:
                porcentaje_hash = (expedientes_con_hash / len(self.expedientes_totales) * 100)
                print(f"‚úì Cobertura hash real: {porcentaje_hash:.1f}%")
            
            # Mostrar algunos ejemplos de hash capturados
            ejemplos_hash = [e.get("hash_uuid_real") for e in self.expedientes_totales if e.get("hash_uuid_real")]
            if ejemplos_hash:
                print(f"‚úì Ejemplos de hash UUID capturados:")
                for i, hash_ejemplo in enumerate(ejemplos_hash[:3], 1):
                    print(f"  {i}. {hash_ejemplo}")
        
        print(f"‚úì Archivos guardados en: {self.salida_dir.absolute()}")
        print(f"\\n{'='*70}")
        print(f"CAPTURA CON HASH REAL COMPLETADA: {datetime.now()}")
        print(f"{'='*70}\\n")


async def main():
    """Funci√≥n principal con extracci√≥n de hash real"""
    scraper = ComprasMXScraper()
    
    # Ejecutar con extracci√≥n de detalles habilitada para capturar hash real
    await scraper.ejecutar(headless=True, extraer_detalles=True)


if __name__ == "__main__":
    asyncio.run(main())
